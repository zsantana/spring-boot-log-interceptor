# ğŸ”¥ DiagnÃ³stico: Request Logger + Kafka - Problemas e SoluÃ§Ãµes

## ğŸš¨ 1) O que acontece quando o Kafka fica indisponÃ­vel?

Depende de como sua LIB envia os logs (sÃ­ncrono vs assÃ­ncrono) e das configuraÃ§Ãµes do KafkaProducer.

### ğŸ”’ Envio sÃ­ncrono (producer.send(...).get() / bloqueante):

- A thread que trata a requisiÃ§Ã£o HTTP fica bloqueada atÃ© a operaÃ§Ã£o de envio terminar (ou timeout)
- Se o Kafka estiver indisponÃ­vel ou o buffer do produtor estiver cheio, a chamada pode bloquear por muito tempo (ou atÃ© `max.block.ms`), aumentando latÃªncia do request
- Threads de servidor (Tomcat/Nio/Undertow/Reactor) podem se esgotar â†’ novas requisiÃ§Ãµes ficam enfileiradas ou rejeitadas

### âš¡ Envio assÃ­ncrono, mas sem isolamento (producer.send(callback) direto na thread de request):

- A chamada `send()` normalmente Ã© rÃ¡pida (enfileira no buffer do produtor), mas se o buffer encher, `send()` pode bloquear atÃ© `max.block.ms`
- Mesmo assincronamente, se vocÃª fizer lÃ³gica de retry/tratamento grande na thread do request, pode bloquear

### âœ… Envio totalmente desacoplado (fila interna + thread(s) produtoras):

- A thread de request sÃ³ empurra um item para uma fila local (preferencialmente bounded). O envio real Ã© feito por threads dedicadas
- Se o Kafka estÃ¡ down, a fila enche; quando cheia, dependendo da polÃ­tica (bloquear/retornar erro/descartar), terÃ¡ efeitos diferentes
- **Esse modelo Ã© muito mais resiliente** â€” evita bloquear threads de request

## ğŸ’¥ 2) Efeitos colaterais prÃ¡ticos (detalhados)

### A. ğŸ§µ ExaustÃ£o do pool de threads HTTP

Quando request-threads ficam bloqueadas esperando envio para Kafka, os workers do container (Tomcat/Jetty/Undertow ou threads do Netty/Reactor) sÃ£o ocupados.

**ConsequÃªncia**: aumento de latÃªncia, timeouts HTTP, erros 503/504, degradaÃ§Ã£o em cascata (clientes re-tentam), e possÃ­vel queda do serviÃ§o.

### B. ğŸ”Œ ExaustÃ£o de pools de conexÃ£o (DB / HTTP / outros recursos)

Threads ocupadas seguram conexÃµes de banco (por exemplo, abrem/retÃªm transaÃ§Ãµes) por mais tempo; o pool de conexÃµes (HikariCP, etc.) pode esgotar.

**Resultado**: operaÃ§Ãµes DB falham ou enfileiram; mais latÃªncia; mais threads esperam por conexÃ£o â†’ efeito cascata.

### C. ğŸ§  Crescimento de memÃ³ria / GC e OOM

Se vocÃª estÃ¡ bufferizando bodies (`ContentCachingRequestWrapper`) e empilhando logs em filas locais, memÃ³ria pode crescer rapidamente com muitos requests e bodies grandes.

Isso leva a mais GC pausas e risco de OOM.

### D. ğŸ“ˆ Aumento de latÃªncia e degradaÃ§Ã£o das SLOs

P95/P99 latÃªncias explodem se threads bloquearem; SLAs sÃ£o afetados.

### E. ğŸŒ ConexÃµes TCP e recursos do Kafka client

O KafkaProducer possui threads de rede prÃ³prias; se Kafka ficar indisponÃ­vel por muito tempo, producer tentarÃ¡ reconectar, mantendo sockets, timers e buffers. Isso consome CPU/memÃ³ria.

**âš ï¸ Importante**: Se criar muitos produtores (ex.: um por request), vocÃª amplifica o problema â€” **nunca crie muitos produtores**.

### F. ğŸ“ PossÃ­vel perda de logs / inconsistÃªncia

Se vocÃª optar por descarte durante falha, perde logs; se reter sem limites, arrisca recursos da app.

## â“ 3) Por que producer.send() pode bloquear?

O produtor mantÃ©m um buffer em memÃ³ria (`buffer.memory`) para mensagens aguardando envio.

Se esse buffer encher (por exemplo, Kafka down), `send()` aguarda espaÃ§o livre â€” bloqueio respeita `max.block.ms`.

ConfiguraÃ§Ãµes de `acks`, `batch.size`, `linger.ms`, `retries` influenciam latÃªncia e memÃ³ria.

## ğŸ—ï¸ 4) Regras gerais de design (boas prÃ¡ticas)

### ğŸ”„ Desacoplar o envio de logs da thread de request:
Use uma fila bounded (ex.: `LinkedBlockingQueue` com tamanho limitado) e threads dedicadas de envio.

### ğŸš« Nunca bloquear indefinidamente a thread de request:
Ao enfileirar, use `offer(item, timeout)` ou `offer(item)` sem bloqueio; se nÃ£o conseguir, fallback (escrever em disco, meter mÃ©trica, descartar com contador).

### ğŸ›¡ï¸ Isolar com Bulkhead / Executor bounded:
Tenha um pool separado e limitado para envio; limite o nÃºmero de mensagens simultÃ¢neas sendo enviadas.

### âš¡ Implementar Circuit Breaker:
Se Kafka falha repetidamente, abrir o circuito e interromper tentativas por um tempo. Use Resilience4j ou Hystrix-like.

### ğŸŒŠ Backpressure / polÃ­ticas de descarte:
PolÃ­ticas: `DROP_OLDEST`, `DROP_NEW`, ou persistÃªncia local.

### ğŸ’¾ PersistÃªncia local como fallback:
Escrever logs em arquivo local (append-only) e um processo de retry off-line que lÃª o arquivo e tenta reentregar.

### ğŸ“Š MÃ©tricas e alertas:
Expor mÃ©tricas: tamanho da fila, taxa de descarte, latÃªncia do envio, erros Kafka.

### ğŸ“ Limitar tamanho do body:
NÃ£o logue bodies gigantescos â€” truncar ou amostrar. Configure limite (ex.: 8KB) e um switch `logBodySamplingRate`.

### â™»ï¸ Reusar um KafkaProducer singleton:
Criar um Ãºnico producer por app (ou pool pequeno), nÃ£o por request.

### âš™ï¸ Configurar producer para evitar bloqueios longos:
- `max.block.ms` (como 5000ms) â€” define tempo mÃ¡ximo que `send()` pode bloquear
- `delivery.timeout.ms`, `request.timeout.ms` controlam timeouts

## ğŸ›ï¸ 5) Exemplo de arquitetura segura (pseudocÃ³digo + comportamento)

```java
// Componente de logging dentro do filter
class RequestLogSender {
  private final BlockingQueue<LogEvent> queue = new LinkedBlockingQueue<>(5000); // bounded
  private final ExecutorService workers = Executors.newFixedThreadPool(2);
  private final KafkaProducer<String, String> producer; // singleton

  public RequestLogSender(KafkaProducer producer) {
    this.producer = producer;
    // iniciar threads consumidoras
    for (int i=0;i<2;i++) {
      workers.submit(this::consumeLoop);
    }
  }

  public boolean submit(LogEvent evt) {
    // tenta enfileirar rapidamente sem bloquear
    boolean accepted = queue.offer(evt);
    if (!accepted) {
      // fila cheia: fallback seguro
      metrics.increment("request_logger.queue_drops");
      // alternativa: gravar em disco append-only para reprocessar
      persistToDisk(evt);
      return false;
    }
    return true;
  }

  private void consumeLoop() {
    while (!Thread.currentThread().isInterrupted()) {
      LogEvent evt = queue.poll(1, TimeUnit.SECONDS);
      if (evt == null) continue;
      try {
        // enviar assÃ­ncrono, sem bloquear a lÃ³gica de consumo (callback apenas registra)
        ProducerRecord<String,String> rec = new ProducerRecord<>("topic-logs", evt.toJson());
        producer.send(rec, (meta, ex) -> {
           if (ex != null) {
             // registrar e talvez persistir para retry
             metrics.increment("kafka.send.errors");
             persistToDisk(evt);
           } else {
             metrics.increment("kafka.send.success");
           }
        });
      } catch (Exception e) {
        // se producer.send lanÃ§ar (buffer cheio e max.block.ms estourado) tratar:
        metrics.increment("kafka.send.exception");
        persistToDisk(evt);
      }
    }
  }
}
```

### ğŸ”§ No filter:

```java
// nÃ£o bloquear: tentar submit e seguir
boolean enqueued = requestLogSender.submit(event);
if (!enqueued) {
  // opcional: adicionar cabeÃ§alho para indicar que a mensagem foi descartada
}
filterChain.doFilter(request, response);
```

## âš™ï¸ 6) ConfiguraÃ§Ãµes Kafka importantes e recomendaÃ§Ãµes

| ConfiguraÃ§Ã£o | DescriÃ§Ã£o | RecomendaÃ§Ã£o |
|---|---|---|
| `buffer.memory` | Total do buffer do producer | Ex.: 32MB. Se baixo, enche rÃ¡pido |
| `max.block.ms` | Tempo mÃ¡ximo que send() bloqueia se buffer cheio | **Defina baixo** (ex.: 5000 ms) |
| `delivery.timeout.ms` | Timeout total para entrega (retries inclusive) | Configure adequadamente |
| `request.timeout.ms` | Timeout por request para lÃ­der | Balancear com latÃªncia |
| `acks` | ConfirmaÃ§Ã£o de entrega | `1` reduz latÃªncia; `all` Ã© mais seguro |
| `retries` e `retry.backoff.ms` | ConfiguraÃ§Ã£o para tolerÃ¢ncia | Configure para resiliÃªncia |
| `linger.ms` e `batch.size` | Para eficiÃªncia | Menos requests com batches maiores |

**âš ï¸ RecomendaÃ§Ã£o**: NÃ£o dependa apenas de configuraÃ§Ãµes Kafka para proteger suas aplicaÃ§Ãµes: combine com o design acima (fila bounded + circuit-breaker + fallback).

## ğŸŒŠ 7) Como isso afeta pools externos (DB, HTTP clients)

- Threads presas seguram conexÃµes DB; transaÃ§Ãµes abertas por mais tempo â†’ Hikari pool exaure â†’ exceÃ§Ãµes ao tentar obter conexÃ£o
- RequisiÃ§Ãµes pendentes tambÃ©m podem manter recursos de sockets abertos
- **Portanto**: qualquer bloqueio na camada HTTP pode causar exaustÃ£o de outros pools â€” **efeito cascata**

## ğŸ”„ 8) EstratÃ©gias de fallback prÃ¡ticas

### ğŸ’¾ PersistÃªncia em disco (append-only) + job de reentrega
Garante durabilidade local.

### âš¡ Circuit Breaker (abrir circuito)
Parar tentativas quando Kafka instÃ¡vel, reduzir carga.

### ğŸ“Š Amostragem (sampling)
Logar sÃ³ 1% dos requests em picos.

### âœ‚ï¸ Truncamento de body
Limite N bytes.

### ğŸš¨ Alertas
Alta taxa de descarte/filas cheias â†’ alertar time de infra.

## ğŸ“ˆ 9) Observabilidade e testes

### ğŸ“Š Expor mÃ©tricas:
- `queue_size`
- `enqueue_timeouts`
- `drop_rate`
- `kafka_send_errors`
- `producer_buffer_utilization`

### ğŸ§ª Teste em chaos scenarios:
Desligue Kafka e veja comportamento da app (stress test).

### â±ï¸ Monitore latÃªncias:
P95/P99 dos endpoints.

## ğŸ“‹ 10) Resumo / recomendaÃ§Ãµes concretas (prÃ¡ticas)

### âŒ Nunca faÃ§a:
- Enviar logs para Kafka diretamente na thread de request de forma bloqueante

### âœ… Sempre faÃ§a:
1. **ğŸ”„ Desacople** com uma fila bounded + threads de envio
2. **âš¡ Implemente** circuit breaker + fallback local (arquivo) e polÃ­tica de descarte
3. **â™»ï¸ Reutilize** um Ãºnico KafkaProducer (singleton) com configuraÃ§Ãµes sensatas (`max.block.ms` baixo)
4. **ğŸ“ Limite** tamanho de body e use amostragem
5. **ğŸ“Š Instrumente** mÃ©tricas e alerte antes de chegar a condiÃ§Ã£o crÃ­tica
6. **ğŸ§ª Teste** o comportamento com Kafka down em ambientes de staging

---

> **ğŸ’¡ Lembre-se**: A resiliÃªncia do sistema depende de como vocÃª lida com falhas, nÃ£o de evitÃ¡-las completamente!